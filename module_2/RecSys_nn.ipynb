{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsolodkyi/RecSystem_SkillBox/blob/main/module_2/RecSys_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeW1KIJ98JrC"
      },
      "source": [
        "# RecSys2019_DeepLearning_Evaluation\n",
        "\n",
        "https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation\n",
        "\n",
        "Установка:\n",
        "pip install -r requirements.txt\n",
        "\n",
        "$ pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "_eky4dpw8rSO",
        "outputId": "841f308a-63c3-4e89-a832-fdc145745f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Cython==0.29.6\n",
            "  Downloading Cython-0.29.6-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting dm-sonnet==1.29\n",
            "  Downloading dm_sonnet-1.29-py3-none-any.whl (640 kB)\n",
            "\u001b[K     |████████████████████████████████| 640 kB 39.5 MB/s \n",
            "\u001b[?25hCollecting h5py==2.9.0\n",
            "  Downloading h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 32.9 MB/s \n",
            "\u001b[?25hCollecting Keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 21.4 MB/s \n",
            "\u001b[?25hCollecting Keras-Applications==1.0.7\n",
            "  Downloading Keras_Applications-1.0.7-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 546 kB/s \n",
            "\u001b[?25hCollecting Keras-Preprocessing==1.0.9\n",
            "  Downloading Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.0.3\n",
            "  Downloading matplotlib-3.0.3-cp37-cp37m-manylinux1_x86_64.whl (13.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0 MB 31.6 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 54.8 MB/s \n",
            "\u001b[?25hCollecting nose==1.3.7\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 42.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.2\n",
            "  Downloading numpy-1.16.2-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 139 kB/s \n",
            "\u001b[?25hCollecting pandas==0.24.2\n",
            "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 28.7 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.20.3\n",
            "  Downloading scikit_learn-0.20.3-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 36.0 MB/s \n",
            "\u001b[?25hCollecting scikit-optimize==0.5.2\n",
            "  Downloading scikit_optimize-0.5.2-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting scipy==1.2.1\n",
            "  Downloading scipy-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (24.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting seaborn==0.9.0\n",
            "  Downloading seaborn-0.9.0-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 54.1 MB/s \n",
            "\u001b[?25hCollecting tensorboard==1.13.1\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 43.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 62 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 45.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-probability==0.6.0\n",
            "  Downloading tensorflow_probability-0.6.0-py2.py3-none-any.whl (790 kB)\n",
            "\u001b[K     |████████████████████████████████| 790 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting tornado==6.0.2\n",
            "  Downloading tornado-6.0.2.tar.gz (481 kB)\n",
            "\u001b[K     |████████████████████████████████| 481 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "  Downloading tqdm-4.31.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting wrapt==1.11.1\n",
            "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet==1.29->-r requirements.txt (line 2)) (0.5.5)\n",
            "Collecting semantic-version\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from dm-sonnet==1.29->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from dm-sonnet==1.29->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 4)) (6.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 7)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.0.3->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/dist-packages (from pandas==0.24.2->-r requirements.txt (line 11)) (2022.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.13.1->-r requirements.txt (line 16)) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.13.1->-r requirements.txt (line 16)) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.13.1->-r requirements.txt (line 16)) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.13.1->-r requirements.txt (line 16)) (1.47.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==1.13.1->-r requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 17)) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 17)) (0.5.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 17)) (1.1.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.0.3->-r requirements.txt (line 7)) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==1.13.1->-r requirements.txt (line 16)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==1.13.1->-r requirements.txt (line 16)) (3.8.1)\n",
            "Building wheels for collected packages: nltk, tornado, wrapt\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449924 sha256=8cc65962fa38bfe5ebb941feaa0dfa9df896d0a0d0ff9ce42043222ea5a9fe44\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tornado: filename=tornado-6.0.2-cp37-cp37m-linux_x86_64.whl size=423907 sha256=3c6a2c9059cf5615af1830f4278d5894c28c89318311cc9d5c80ae5c42f9439f\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/ec/4d/1aca4b143f2a19439632329f1d5b6b9f62ee9508ddf30f9d2c\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68442 sha256=7f3f65a5c7aa627ccf0bbe9d07ed17c5661e60d4a698ea98ed25e43136289f63\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
            "Successfully built nltk tornado wrapt\n",
            "Installing collected packages: numpy, scipy, mock, h5py, wrapt, tensorflow-estimator, tensorboard, semantic-version, scikit-learn, pandas, matplotlib, Keras-Preprocessing, Keras-Applications, tqdm, tornado, tensorflow-probability, tensorflow, seaborn, scikit-optimize, nose, nltk, Keras, dm-sonnet, Cython\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: Keras-Preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.16.0\n",
            "    Uninstalling tensorflow-probability-0.16.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.16.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.11.2\n",
            "    Uninstalling seaborn-0.11.2:\n",
            "      Successfully uninstalled seaborn-0.11.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: Keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.32\n",
            "    Uninstalling Cython-0.29.32:\n",
            "      Successfully uninstalled Cython-0.29.32\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.20.3 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.16.2 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 0.24.2 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.16.2 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.2 which is incompatible.\n",
            "spacy 3.4.1 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.16.2 which is incompatible.\n",
            "resampy 0.4.0 requires numpy>=1.17, but you have numpy 1.16.2 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.16.2 which is incompatible.\n",
            "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.2.1 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.2 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.2 which is incompatible.\n",
            "prophet 1.1 requires pandas>=1.0.4, but you have pandas 0.24.2 which is incompatible.\n",
            "prophet 1.1 requires tqdm>=4.36.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "plotnine 0.8.0 requires matplotlib>=3.1.1, but you have matplotlib 3.0.3 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.16.2 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 0.24.2 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.2.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "numba 0.56.0 requires numpy<1.23,>=1.18, but you have numpy 1.16.2 which is incompatible.\n",
            "mizani 0.7.3 requires matplotlib>=3.1.1, but you have matplotlib 3.0.3 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 0.24.2 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.16.2 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.2.1 which is incompatible.\n",
            "jax 0.3.14 requires numpy>=1.19, but you have numpy 1.16.2 which is incompatible.\n",
            "jax 0.3.14 requires scipy>=1.5, but you have scipy 1.2.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.3 which is incompatible.\n",
            "gym 0.25.1 requires numpy>=1.18.0, but you have numpy 1.16.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 0.24.2 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.0.2 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.16.2 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "Successfully installed Cython-0.29.6 Keras-2.2.4 Keras-Applications-1.0.7 Keras-Preprocessing-1.0.9 dm-sonnet-1.29 h5py-2.9.0 matplotlib-3.0.3 mock-4.0.3 nltk-3.4.5 nose-1.3.7 numpy-1.16.2 pandas-0.24.2 scikit-learn-0.20.3 scikit-optimize-0.5.2 scipy-1.2.1 seaborn-0.9.0 semantic-version-2.10.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 tensorflow-probability-0.6.0 tornado-6.0.2 tqdm-4.31.1 wrapt-1.11.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxf9_72N8JrM"
      },
      "source": [
        "## Neural Collaborative Filtering\n",
        "https://dl.acm.org/doi/pdf/10.1145/3038912.3052569"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek8CeHYU8JrN",
        "outputId": "7653d47a-a0c9-46e8-9f88-5fe5ad9a0bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ef2618fe47d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseRecommender\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseRecommender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncremental_Training_Early_Stopping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIncremental_Training_Early_Stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Base'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from Base.BaseRecommender import BaseRecommender\n",
        "from Base.Incremental_Training_Early_Stopping import Incremental_Training_Early_Stopping\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sps\n",
        "from Base.DataIO import DataIO\n",
        "import os\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.models import Model, load_model, save_model, clone_model\n",
        "from keras.layers import Embedding, Input, Dense, Reshape, Flatten, Dropout, Concatenate, Multiply\n",
        "from keras.optimizers import Adagrad, Adam, SGD, RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io5XW6U08JrP"
      },
      "outputs": [],
      "source": [
        "def MLP_get_model(num_users, num_items, layers = [20,10], reg_layers=[0,0]):\n",
        "    assert len(layers) == len(reg_layers)\n",
        "    num_layer = len(layers) #Number of layers in the MLP\n",
        "    # Input variables\n",
        "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
        "\n",
        "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = 'user_embedding',\n",
        "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
        "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'item_embedding',\n",
        "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
        "\n",
        "    # Crucial to flatten an embedding vector!\n",
        "    user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
        "    item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
        "\n",
        "    # The 0-th layer is the concatenation of embedding layers\n",
        "    vector = Concatenate()([user_latent, item_latent])\n",
        "\n",
        "    # MLP layers\n",
        "    for idx in range(1, num_layer):\n",
        "        layer = Dense(layers[idx], kernel_regularizer = l2(reg_layers[idx]), activation='relu', name = 'layer%d' %idx)\n",
        "        vector = layer(vector)\n",
        "\n",
        "    # Final prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(vector)\n",
        "\n",
        "    model = Model(inputs=[user_input, item_input],\n",
        "                  outputs=prediction)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def GMF_get_model(num_users, num_items, latent_dim, regs=[0,0]):\n",
        "    # Input variables\n",
        "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
        "\n",
        "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding',\n",
        "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(regs[0]), input_length=1)\n",
        "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding',\n",
        "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(regs[1]), input_length=1)\n",
        "\n",
        "    # Crucial to flatten an embedding vector!\n",
        "    user_latent = Flatten()(MF_Embedding_User(user_input))\n",
        "    item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
        "\n",
        "    # Element-wise product of user and item embeddings\n",
        "    predict_vector = Multiply()([user_latent, item_latent])\n",
        "\n",
        "    # Final prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(predict_vector)\n",
        "\n",
        "    model = Model(inputs=[user_input, item_input],\n",
        "                outputs=prediction)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def NeuCF_get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0.0):\n",
        "    assert len(layers) == len(reg_layers)\n",
        "    num_layer = len(layers) #Number of layers in the MLP\n",
        "    # Input variables\n",
        "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
        "\n",
        "    # Embedding layer\n",
        "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
        "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_mf), input_length=1)\n",
        "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
        "                                  embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_mf), input_length=1)\n",
        "\n",
        "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = \"mlp_embedding_user\",\n",
        "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
        "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item',\n",
        "                                   embeddings_initializer = 'random_normal', embeddings_regularizer = l2(reg_layers[0]), input_length=1)\n",
        "\n",
        "    # MF part\n",
        "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
        "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
        "    mf_vector = Multiply()([mf_user_latent, mf_item_latent]) # element-wise multiply\n",
        "\n",
        "    # MLP part\n",
        "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
        "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
        "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
        "    for idx in range(1, num_layer):\n",
        "        layer = Dense(layers[idx], kernel_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
        "        mlp_vector = layer(mlp_vector)\n",
        "\n",
        "    # Concatenate MF and MLP parts\n",
        "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
        "\n",
        "    # Final prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = \"prediction\")(predict_vector)\n",
        "\n",
        "    model = Model(inputs=[user_input, item_input],\n",
        "                  outputs=prediction)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def load_pretrain_model(model, gmf_model, mlp_model, num_layers):\n",
        "    # MF embeddings\n",
        "    gmf_user_embeddings = gmf_model.get_layer('user_embedding').get_weights()\n",
        "    gmf_item_embeddings = gmf_model.get_layer('item_embedding').get_weights()\n",
        "    model.get_layer('mf_embedding_user').set_weights(gmf_user_embeddings)\n",
        "    model.get_layer('mf_embedding_item').set_weights(gmf_item_embeddings)\n",
        "\n",
        "    # MLP embeddings\n",
        "    mlp_user_embeddings = mlp_model.get_layer('user_embedding').get_weights()\n",
        "    mlp_item_embeddings = mlp_model.get_layer('item_embedding').get_weights()\n",
        "    model.get_layer('mlp_embedding_user').set_weights(mlp_user_embeddings)\n",
        "    model.get_layer('mlp_embedding_item').set_weights(mlp_item_embeddings)\n",
        "\n",
        "    # MLP layers\n",
        "    for i in range(1, num_layers):\n",
        "        mlp_layer_weights = mlp_model.get_layer('layer%d' %i).get_weights()\n",
        "        model.get_layer('layer%d' %i).set_weights(mlp_layer_weights)\n",
        "\n",
        "    # Prediction weights\n",
        "    gmf_prediction = gmf_model.get_layer('prediction').get_weights()\n",
        "    mlp_prediction = mlp_model.get_layer('prediction').get_weights()\n",
        "    new_weights = np.concatenate((gmf_prediction[0], mlp_prediction[0]), axis=0)\n",
        "    new_b = gmf_prediction[1] + mlp_prediction[1]\n",
        "    model.get_layer('prediction').set_weights([0.5*new_weights, 0.5*new_b])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_train_instances(train, num_negatives, num_items):\n",
        "    user_input, item_input, labels = [],[],[]\n",
        "    num_users = train.shape[0]\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        labels.append(1)\n",
        "        # negative instances\n",
        "        for t in range(num_negatives):\n",
        "            j = np.random.randint(num_items)\n",
        "            while (u, j) in train.keys():#train.has_key((u, j)):\n",
        "                j = np.random.randint(num_items)\n",
        "            user_input.append(u)\n",
        "            item_input.append(j)\n",
        "            labels.append(0)\n",
        "    return user_input, item_input, labels\n",
        "\n",
        "\n",
        "\n",
        "def set_learner(model, learning_rate, learner):\n",
        "\n",
        "    if learner.lower() == \"adagrad\":\n",
        "        model.compile(optimizer=Adagrad(lr=learning_rate), loss='binary_crossentropy')\n",
        "    elif learner.lower() == \"rmsprop\":\n",
        "        model.compile(optimizer=RMSprop(lr=learning_rate), loss='binary_crossentropy')\n",
        "    elif learner.lower() == \"adam\":\n",
        "        model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy')\n",
        "    else:\n",
        "        model.compile(optimizer=SGD(lr=learning_rate), loss='binary_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def deep_clone_model(source_model):\n",
        "\n",
        "    destination_model = clone_model(source_model)\n",
        "    destination_model.set_weights(source_model.get_weights())\n",
        "\n",
        "    return destination_model\n",
        "\n",
        "\n",
        "\n",
        "class NeuMF_RecommenderWrapper(BaseRecommender, Incremental_Training_Early_Stopping):\n",
        "\n",
        "\n",
        "    RECOMMENDER_NAME = \"NeuMF_RecommenderWrapper\"\n",
        "\n",
        "    def __init__(self, URM_train):\n",
        "        super(NeuMF_RecommenderWrapper, self).__init__(URM_train)\n",
        "\n",
        "        self._train = sps.dok_matrix(self.URM_train)\n",
        "        self.n_users, self.n_items = self.URM_train.shape\n",
        "\n",
        "        self._item_indices = np.arange(0, self.n_items, dtype=np.int)\n",
        "        self._user_ones_vector = np.ones_like(self._item_indices)\n",
        "\n",
        "\n",
        "    def _compute_item_score(self, user_id_array, items_to_compute=None):\n",
        "\n",
        "        item_scores = - np.ones((len(user_id_array), self.n_items)) * np.inf\n",
        "\n",
        "        for user_index in range(len(user_id_array)):\n",
        "\n",
        "            user_id = user_id_array[user_index]\n",
        "\n",
        "            # The prediction requires a list of two arrays user_id, item_id of equal length\n",
        "            # To compute the recommendations for a single user, we must provide its index as many times as the\n",
        "            # number of items\n",
        "            item_score_user = self.model.predict([self._user_ones_vector*user_id, self._item_indices],\n",
        "                                                 batch_size=100, verbose=0)\n",
        "\n",
        "\n",
        "            if items_to_compute is not None:\n",
        "                item_scores[user_index, items_to_compute] = item_score_user.ravel()[items_to_compute]\n",
        "            else:\n",
        "                item_scores[user_index, :] = item_score_user.ravel()\n",
        "\n",
        "\n",
        "        return item_scores\n",
        "\n",
        "\n",
        "    def get_early_stopping_final_epochs_dict(self):\n",
        "        \"\"\"\n",
        "        This function returns a dictionary to be used as optimal parameters in the .fit() function\n",
        "        It provides the flexibility to deal with multiple early-stopping in a single algorithm\n",
        "        e.g. in NeuMF there are three model componets each with its own optimal number of epochs\n",
        "        the return dict would be {\"epochs\": epochs_best_neumf, \"epochs_gmf\": epochs_best_gmf, \"epochs_mlp\": epochs_best_mlp}\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        return {\"epochs\": self.epochs_best, \"epochs_gmf\": self.epochs_best_gmf, \"epochs_mlp\": self.epochs_best_mlp}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self,\n",
        "            epochs = 100,\n",
        "            epochs_gmf=100,\n",
        "            epochs_mlp=100,\n",
        "            batch_size = 256,\n",
        "            num_factors = 8,\n",
        "            layers = [64,32,16,8],\n",
        "            reg_mf = 0.0,\n",
        "            reg_layers = [0,0,0,0],\n",
        "            num_negatives = 4,\n",
        "            learning_rate = 1e-3,\n",
        "            learning_rate_pretrain = 1e-3,\n",
        "            learner = 'sgd',\n",
        "            learner_pretrain = 'adam',\n",
        "            pretrain = True,\n",
        "            root_folder_pretrain = None,\n",
        "            **earlystopping_kwargs):\n",
        "        \"\"\"\n",
        "\n",
        "        :param epochs:\n",
        "        :param batch_size:\n",
        "        :param num_factors: Embedding size of MF model\n",
        "        :param layers: MLP layers. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\n",
        "        :param reg_mf: Regularization for MF embeddings.\n",
        "        :param reg_layers: Regularization for each MLP layer. reg_layers[0] is the regularization for embeddings.\n",
        "        :param num_negatives: Number of negative instances to pair with a positive instance.\n",
        "        :param learning_rate:\n",
        "        :param learning_rate_pretrain:\n",
        "        :param learner: adagrad, adam, rmsprop, sgd\n",
        "        :param learner_pretrain: adagrad, adam, rmsprop, sgd\n",
        "        :param root_folder_pretrain: Specify the pretrain model folder where to save MF and MLP for MF part.\n",
        "        :param do_pretrain:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.mf_dim = num_factors\n",
        "        self.layers = layers.copy()\n",
        "        self.reg_mf = reg_mf\n",
        "        self.reg_layers = reg_layers.copy()\n",
        "        self.num_negatives = num_negatives\n",
        "\n",
        "        assert learner in [\"adagrad\", \"adam\", \"rmsprop\", \"sgd\"]\n",
        "        assert learner_pretrain in [\"adagrad\", \"adam\", \"rmsprop\", \"sgd\"]\n",
        "\n",
        "        self.pretrain = pretrain\n",
        "\n",
        "        if self.pretrain:\n",
        "\n",
        "            if root_folder_pretrain is not None:\n",
        "                print(\"NeuMF_RecommenderWrapper: pretrained models will be saved in '{}'\".format(root_folder_pretrain))\n",
        "\n",
        "                # If directory does not exist, create\n",
        "                if not os.path.exists(root_folder_pretrain):\n",
        "                    os.makedirs(root_folder_pretrain)\n",
        "\n",
        "            print(\"NeuMF_RecommenderWrapper: root_folder_pretrain not provided, pretrained models will not be saved\")\n",
        "\n",
        "            print(\"NeuMF_RecommenderWrapper: Pretraining GMF...\")\n",
        "\n",
        "            self.model = GMF_get_model(self.n_users, self.n_items, self.mf_dim)\n",
        "            self.model = set_learner(self.model, learning_rate_pretrain, learner_pretrain)\n",
        "\n",
        "            self._best_model = deep_clone_model(self.model)\n",
        "\n",
        "            self._train_with_early_stopping(epochs_gmf,\n",
        "                                            algorithm_name = self.RECOMMENDER_NAME,\n",
        "                                            **earlystopping_kwargs)\n",
        "\n",
        "            self.epochs_best_gmf = self.epochs_best\n",
        "\n",
        "            if root_folder_pretrain is not None:\n",
        "                model_out_file = \"GMF_factors_{}_pretrain\".format(self.mf_dim)\n",
        "                self._best_model.save_weights(root_folder_pretrain + model_out_file, overwrite=True)\n",
        "\n",
        "            self.gmf_model = deep_clone_model(self._best_model)\n",
        "\n",
        "\n",
        "\n",
        "            print(\"NeuMF_RecommenderWrapper: Pretraining MLP...\")\n",
        "\n",
        "            self.model = MLP_get_model(self.n_users, self.n_items, self.layers, self.reg_layers)\n",
        "            self.model = set_learner(self.model, learning_rate_pretrain, learner_pretrain)\n",
        "\n",
        "            self._best_model = deep_clone_model(self.model)\n",
        "\n",
        "            self._train_with_early_stopping(epochs_mlp,\n",
        "                                            algorithm_name = self.RECOMMENDER_NAME,\n",
        "                                            **earlystopping_kwargs)\n",
        "\n",
        "            self.epochs_best_mlp = self.epochs_best\n",
        "\n",
        "            if root_folder_pretrain is not None:\n",
        "                model_out_file = \"MLP_layers_{}_reg_layers_{}_pretrain\".format(self.layers, reg_layers)\n",
        "                self._best_model.save_weights(root_folder_pretrain + model_out_file, overwrite=True)\n",
        "\n",
        "            self.mlp_model = deep_clone_model(self._best_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Build model\n",
        "        self.model = NeuCF_get_model(self.n_users, self.n_items, self.mf_dim, self.layers, self.reg_layers, self.reg_mf)\n",
        "        self.model = set_learner(self.model, learning_rate, learner)\n",
        "\n",
        "\n",
        "        # Load pretrain model\n",
        "        if pretrain:\n",
        "            self.model = load_pretrain_model(self.model, self.gmf_model, self.mlp_model, len(layers))\n",
        "            print(\"NeuMF_RecommenderWrapper: Load pretrained GMF and MLP models.\")\n",
        "\n",
        "\n",
        "        print(\"NeuMF_RecommenderWrapper: Training NeuCF...\")\n",
        "\n",
        "        self._best_model = deep_clone_model(self.model)\n",
        "\n",
        "        self._train_with_early_stopping(epochs,\n",
        "                                        algorithm_name = self.RECOMMENDER_NAME,\n",
        "                                        **earlystopping_kwargs)\n",
        "\n",
        "\n",
        "        print(\"NeuMF_RecommenderWrapper: Tranining complete\")\n",
        "\n",
        "        self.model = deep_clone_model(self._best_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _prepare_model_for_validation(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def _update_best_model(self):\n",
        "        # Keras only clones the structure of the model, not the weights\n",
        "        self._best_model = deep_clone_model(self.model)\n",
        "\n",
        "\n",
        "    def _run_epoch(self, currentEpoch):\n",
        "\n",
        "        # Generate training instances\n",
        "        user_input, item_input, labels = get_train_instances(self._train, self.num_negatives, self.n_items)\n",
        "\n",
        "        # Training\n",
        "        hist = self.model.fit([np.array(user_input), np.array(item_input)], #input\n",
        "                         np.array(labels), # labels\n",
        "                         batch_size=self.batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "\n",
        "        print(\"NeuMF_RecommenderWrapper: Epoch {}, loss {:.2E}\".format(currentEpoch+1, hist.history['loss'][0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self, folder_path, file_name = None):\n",
        "\n",
        "        if file_name is None:\n",
        "            file_name = self.RECOMMENDER_NAME\n",
        "\n",
        "        self._print(\"Saving model in file '{}'\".format(folder_path + file_name))\n",
        "\n",
        "        self.model.save_weights(folder_path + file_name + \"_weights\", overwrite=True)\n",
        "\n",
        "        data_dict_to_save = {\n",
        "            \"n_users\": self.n_users,\n",
        "            \"n_items\": self.n_items,\n",
        "            \"mf_dim\": self.mf_dim,\n",
        "            \"layers\": self.layers,\n",
        "            \"reg_layers\": self.reg_layers,\n",
        "            \"reg_mf\": self.reg_mf,\n",
        "        }\n",
        "\n",
        "        dataIO = DataIO(folder_path=folder_path)\n",
        "        dataIO.save_data(file_name=file_name, data_dict_to_save = data_dict_to_save)\n",
        "\n",
        "\n",
        "        self._print(\"Saving complete\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def load_model(self, folder_path, file_name = None):\n",
        "\n",
        "        if file_name is None:\n",
        "            file_name = self.RECOMMENDER_NAME\n",
        "\n",
        "        self._print(\"Loading model from file '{}'\".format(folder_path + file_name))\n",
        "\n",
        "        dataIO = DataIO(folder_path=folder_path)\n",
        "        data_dict = dataIO.load_data(file_name=file_name)\n",
        "\n",
        "        for attrib_name in data_dict.keys():\n",
        "             self.__setattr__(attrib_name, data_dict[attrib_name])\n",
        "\n",
        "\n",
        "        self.model = NeuCF_get_model(self.n_users, self.n_items, self.mf_dim, self.layers, self.reg_layers, self.reg_mf)\n",
        "        self.model.load_weights(folder_path + file_name + \"_weights\")\n",
        "\n",
        "\n",
        "        self._print(\"Loading complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAFwFWEP8JrU"
      },
      "source": [
        "## run_WWW_17_NeuMF.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlatW8Wj8JrU"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on 22/11/17\n",
        "\n",
        "@author: Maurizio Ferrari Dacrema\n",
        "\"\"\"\n",
        "\n",
        "from Recommender_import_list import *\n",
        "from Conferences.WWW.NeuMF_our_interface.NeuMF_RecommenderWrapper import NeuMF_RecommenderWrapper\n",
        "\n",
        "\n",
        "from ParameterTuning.run_parameter_search import runParameterSearch_Collaborative\n",
        "from ParameterTuning.SearchSingleCase import SearchSingleCase\n",
        "from ParameterTuning.SearchAbstractClass import SearchInputRecommenderArgs\n",
        "\n",
        "import os, traceback, argparse\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "from Utils.assertions_on_data_for_experiments import assert_implicit_data, assert_disjoint_matrices\n",
        "from Utils.plot_popularity import plot_popularity_bias, save_popularity_statistics\n",
        "\n",
        "\n",
        "from Utils.ResultFolderLoader import ResultFolderLoader, generate_latex_hyperparameters\n",
        "\n",
        "\n",
        "\n",
        "def read_data_split_and_search(dataset_name,\n",
        "                                   flag_baselines_tune = False,\n",
        "                                   flag_DL_article_default = False, flag_DL_tune = False,\n",
        "                                   flag_print_results = False):\n",
        "\n",
        "\n",
        "    from Conferences.WWW.NeuMF_our_interface.Movielens1M.Movielens1MReader import Movielens1MReader\n",
        "    from Conferences.WWW.NeuMF_our_interface.Pinterest.PinterestICCVReader import PinterestICCVReader\n",
        "\n",
        "\n",
        "    result_folder_path = \"result_experiments/{}/{}_{}/\".format(CONFERENCE_NAME, ALGORITHM_NAME, dataset_name)\n",
        "\n",
        "\n",
        "    if dataset_name == \"movielens1m\":\n",
        "        dataset = Movielens1MReader(result_folder_path)\n",
        "\n",
        "    elif dataset_name == \"pinterest\":\n",
        "        dataset = PinterestICCVReader(result_folder_path)\n",
        "\n",
        "\n",
        "    URM_train = dataset.URM_DICT[\"URM_train\"].copy()\n",
        "    URM_validation = dataset.URM_DICT[\"URM_validation\"].copy()\n",
        "    URM_test = dataset.URM_DICT[\"URM_test\"].copy()\n",
        "    URM_test_negative = dataset.URM_DICT[\"URM_test_negative\"].copy()\n",
        "\n",
        "\n",
        "    # Ensure IMPLICIT data and DISJOINT sets\n",
        "    assert_implicit_data([URM_train, URM_validation, URM_test, URM_test_negative])\n",
        "\n",
        "    assert_disjoint_matrices([URM_train, URM_validation, URM_test])\n",
        "    assert_disjoint_matrices([URM_train, URM_validation, URM_test_negative])\n",
        "\n",
        "\n",
        "\n",
        "    # If directory does not exist, create\n",
        "    if not os.path.exists(result_folder_path):\n",
        "        os.makedirs(result_folder_path)\n",
        "\n",
        "\n",
        "\n",
        "    algorithm_dataset_string = \"{}_{}_\".format(ALGORITHM_NAME, dataset_name)\n",
        "\n",
        "    plot_popularity_bias([URM_train + URM_validation, URM_test],\n",
        "                         [\"URM train\", \"URM test\"],\n",
        "                         result_folder_path + algorithm_dataset_string + \"popularity_plot\")\n",
        "\n",
        "    save_popularity_statistics([URM_train + URM_validation, URM_test],\n",
        "                               [\"URM train\", \"URM test\"],\n",
        "                               result_folder_path + algorithm_dataset_string + \"popularity_statistics\")\n",
        "\n",
        "\n",
        "\n",
        "    collaborative_algorithm_list = [\n",
        "        Random,\n",
        "        TopPop,\n",
        "        UserKNNCFRecommender,\n",
        "        ItemKNNCFRecommender,\n",
        "        P3alphaRecommender,\n",
        "        RP3betaRecommender,\n",
        "        PureSVDRecommender,\n",
        "        NMFRecommender,\n",
        "        IALSRecommender,\n",
        "        MatrixFactorization_BPR_Cython,\n",
        "        MatrixFactorization_FunkSVD_Cython,\n",
        "        EASE_R_Recommender,\n",
        "        SLIM_BPR_Cython,\n",
        "        SLIMElasticNetRecommender,\n",
        "        ]\n",
        "\n",
        "    metric_to_optimize = \"HIT_RATE\"\n",
        "    n_cases = 50\n",
        "    n_random_starts = 15\n",
        "\n",
        "\n",
        "    from Base.Evaluation.Evaluator import EvaluatorNegativeItemSample\n",
        "\n",
        "    evaluator_validation = EvaluatorNegativeItemSample(URM_validation, URM_test_negative, cutoff_list=[10])\n",
        "    evaluator_test = EvaluatorNegativeItemSample(URM_test, URM_test_negative, cutoff_list=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "\n",
        "    runParameterSearch_Collaborative_partial = partial(runParameterSearch_Collaborative,\n",
        "                                                       URM_train = URM_train,\n",
        "                                                       URM_train_last_test = URM_train + URM_validation,\n",
        "                                                       metric_to_optimize = metric_to_optimize,\n",
        "                                                       evaluator_validation_earlystopping = evaluator_validation,\n",
        "                                                       evaluator_validation = evaluator_validation,\n",
        "                                                       evaluator_test = evaluator_test,\n",
        "                                                       output_folder_path = result_folder_path,\n",
        "                                                       parallelizeKNN = False,\n",
        "                                                       allow_weighting = True,\n",
        "                                                       resume_from_saved = True,\n",
        "                                                       n_cases = n_cases,\n",
        "                                                       n_random_starts = n_random_starts)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if flag_baselines_tune:\n",
        "\n",
        "        for recommender_class in collaborative_algorithm_list:\n",
        "            try:\n",
        "                runParameterSearch_Collaborative_partial(recommender_class)\n",
        "            except Exception as e:\n",
        "                print(\"On recommender {} Exception {}\".format(recommender_class, str(e)))\n",
        "                traceback.print_exc()\n",
        "\n",
        "\n",
        "\n",
        "    ################################################################################################\n",
        "    ######\n",
        "    ######      DL ALGORITHM\n",
        "    ######\n",
        "\n",
        "    if flag_DL_article_default:\n",
        "\n",
        "        try:\n",
        "\n",
        "\n",
        "            if dataset_name == \"movielens1m\":\n",
        "                num_factors = 64\n",
        "            elif dataset_name == \"pinterest\":\n",
        "                num_factors = 16\n",
        "\n",
        "\n",
        "            neuMF_article_hyperparameters = {\n",
        "                \"epochs\": 100,\n",
        "                \"epochs_gmf\": 100,\n",
        "                \"epochs_mlp\": 100,\n",
        "                \"batch_size\": 256,\n",
        "                \"num_factors\": num_factors,\n",
        "                \"layers\": [num_factors*4, num_factors*2, num_factors],\n",
        "                \"reg_mf\": 0.0,\n",
        "                \"reg_layers\": [0,0,0],\n",
        "                \"num_negatives\": 4,\n",
        "                \"learning_rate\": 1e-3,\n",
        "                \"learning_rate_pretrain\": 1e-3,\n",
        "                \"learner\": \"sgd\",\n",
        "                \"learner_pretrain\": \"adam\",\n",
        "                \"pretrain\": True\n",
        "            }\n",
        "\n",
        "\n",
        "            neuMF_earlystopping_hyperparameters = {\n",
        "                \"validation_every_n\": 5,\n",
        "                \"stop_on_validation\": True,\n",
        "                \"evaluator_object\": evaluator_validation,\n",
        "                \"lower_validations_allowed\": 5,\n",
        "                \"validation_metric\": metric_to_optimize\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "            parameterSearch = SearchSingleCase(NeuMF_RecommenderWrapper,\n",
        "                                               evaluator_validation=evaluator_validation,\n",
        "                                               evaluator_test=evaluator_test)\n",
        "\n",
        "            recommender_input_args = SearchInputRecommenderArgs(\n",
        "                                                CONSTRUCTOR_POSITIONAL_ARGS = [URM_train],\n",
        "                                                FIT_KEYWORD_ARGS = neuMF_earlystopping_hyperparameters)\n",
        "\n",
        "            recommender_input_args_last_test = recommender_input_args.copy()\n",
        "            recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train + URM_validation\n",
        "\n",
        "            parameterSearch.search(recommender_input_args,\n",
        "                                   recommender_input_args_last_test = recommender_input_args_last_test,\n",
        "                                   fit_hyperparameters_values=neuMF_article_hyperparameters,\n",
        "                                   output_folder_path = result_folder_path,\n",
        "                                   resume_from_saved = True,\n",
        "                                   output_file_name_root = NeuMF_RecommenderWrapper.RECOMMENDER_NAME)\n",
        "\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            print(\"On recommender {} Exception {}\".format(NeuMF_RecommenderWrapper, str(e)))\n",
        "            traceback.print_exc()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################################################################################################\n",
        "    ######\n",
        "    ######      PRINT RESULTS\n",
        "    ######\n",
        "\n",
        "    if flag_print_results:\n",
        "\n",
        "        n_test_users = np.sum(np.ediff1d(URM_test.indptr)>=1)\n",
        "        file_name = \"{}..//{}_{}_\".format(result_folder_path, ALGORITHM_NAME, dataset_name)\n",
        "\n",
        "        result_loader = ResultFolderLoader(result_folder_path,\n",
        "                                         base_algorithm_list = None,\n",
        "                                         other_algorithm_list = [NeuMF_RecommenderWrapper],\n",
        "                                         KNN_similarity_list = KNN_similarity_to_report_list,\n",
        "                                         ICM_names_list = None,\n",
        "                                         UCM_names_list = None)\n",
        "\n",
        "\n",
        "        result_loader.generate_latex_results(file_name + \"{}_latex_results.txt\".format(\"article_metrics\"),\n",
        "                                           metrics_list = [\"HIT_RATE\", \"NDCG\"],\n",
        "                                           cutoffs_list = [1, 5, 10],\n",
        "                                           table_title = None,\n",
        "                                           highlight_best = True)\n",
        "\n",
        "        result_loader.generate_latex_results(file_name + \"{}_latex_results.txt\".format(\"all_metrics\"),\n",
        "                                           metrics_list = [\"PRECISION\", \"RECALL\", \"MAP\", \"MRR\", \"NDCG\", \"F1\", \"HIT_RATE\", \"ARHR\",\n",
        "                                                           \"NOVELTY\", \"DIVERSITY_MEAN_INTER_LIST\", \"DIVERSITY_HERFINDAHL\", \"COVERAGE_ITEM\", \"DIVERSITY_GINI\", \"SHANNON_ENTROPY\"],\n",
        "                                           cutoffs_list = [10],\n",
        "                                           table_title = None,\n",
        "                                           highlight_best = True)\n",
        "\n",
        "        result_loader.generate_latex_time_statistics(file_name + \"{}_latex_results.txt\".format(\"time\"),\n",
        "                                           n_evaluation_users=n_test_users,\n",
        "                                           table_title = None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ALGORITHM_NAME = \"NeuMF\"\n",
        "    CONFERENCE_NAME = \"WWW\"\n",
        "\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-b', '--baseline_tune',        help=\"Baseline hyperparameter search\", type = bool, default = False)\n",
        "    parser.add_argument('-a', '--DL_article_default',   help=\"Train the DL model with article hyperparameters\", type = bool, default = False)\n",
        "    parser.add_argument('-p', '--print_results',        help=\"Print results\", type = bool, default = True)\n",
        "\n",
        "    input_flags = parser.parse_args()\n",
        "    print(input_flags)\n",
        "\n",
        "    KNN_similarity_to_report_list = [\"cosine\", \"dice\", \"jaccard\", \"asymmetric\", \"tversky\"]\n",
        "\n",
        "\n",
        "\n",
        "    dataset_list = [\"movielens1m\", \"pinterest\"]\n",
        "    dataset_list = [\"movielens1m\"] #отредактировано\n",
        "\n",
        "    for dataset_name in dataset_list:\n",
        "\n",
        "        read_data_split_and_search(dataset_name,\n",
        "                                        flag_baselines_tune=input_flags.baseline_tune,\n",
        "                                        flag_DL_article_default= input_flags.DL_article_default,\n",
        "                                        flag_print_results = input_flags.print_results,\n",
        "                                        )\n",
        "\n",
        "\n",
        "\n",
        "    if input_flags.print_results:\n",
        "        generate_latex_hyperparameters(result_folder_path =\"result_experiments/{}/\".format(CONFERENCE_NAME),\n",
        "                                       algorithm_name= ALGORITHM_NAME,\n",
        "                                       experiment_subfolder_list = dataset_list,\n",
        "                                       other_algorithm_list = [NeuMF_RecommenderWrapper],\n",
        "                                       KNN_similarity_to_report_list = KNN_similarity_to_report_list,\n",
        "                                       split_per_algorithm_type = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qzNhdlV8JrW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "RecSys_nn.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}